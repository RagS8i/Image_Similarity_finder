{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df8262-67b2-49e1-9dc8-0d04906ca122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9482645a-5963-4c37-8e78-3c3818d00d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# CLIP Semantic Similarity Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import cv2\n",
    "\n",
    "# 1. Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_proc  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 2. Function to get 512-dim CLIP embedding from image\n",
    "def get_clip_image_emb(image_path: str) -> np.ndarray:\n",
    "    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    inputs = clip_proc(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.get_image_features(**inputs)\n",
    "    emb = emb[0].cpu().numpy()\n",
    "    return emb / (np.linalg.norm(emb) + 1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5cc96-a651-4c83-8db0-92b7fba51efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Semantic Similarity: 87.76%\n"
     ]
    }
   ],
   "source": [
    "# Example usage — CLIP global similarity\n",
    "path1 = \"image_pairs/boy1.jpg\"\n",
    "path2 = \"image_pairs/boy2.jpg\"\n",
    "\n",
    "emb1 = get_clip_image_emb(path1)\n",
    "emb2 = get_clip_image_emb(path2)\n",
    "\n",
    "clip_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "print(f\"CLIP Semantic Similarity: {clip_sim * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d79f116-8c3d-4099-9040-9811fadd7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use YOLO to crop objects and get CLIP embeddings for each crop\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load YOLOv8 (if not already)\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "def get_clip_crops_embeddings(image_path):\n",
    "    \"\"\"\n",
    "    Detects objects using YOLO, returns list of CLIP embeddings for each object crop.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    crops = []\n",
    "    for box in results[0].boxes.xyxy.cpu().numpy():\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        # convert BGR to RGB for CLIP\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        inputs = clip_proc(images=crop_rgb, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = clip_model.get_image_features(**inputs)[0].cpu().numpy()\n",
    "        emb = emb / (np.linalg.norm(emb) + 1e-10)\n",
    "        crops.append(emb)\n",
    "\n",
    "    return crops  # List of 512-dim np arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52fe4e8-a7c4-4566-82cb-62ed9d69a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_object_level_similarity(embs1, embs2):\n",
    "    \"\"\"\n",
    "    Matches each embedding in embs1 to its most similar in embs2.\n",
    "    Returns average best-match similarity.\n",
    "    \"\"\"\n",
    "    if not embs1 or not embs2:\n",
    "        return 0.0\n",
    "\n",
    "    total_sim = 0\n",
    "    for e1 in embs1:\n",
    "        sims = [cosine_similarity([e1], [e2])[0][0] for e2 in embs2]\n",
    "        total_sim += max(sims)  # best match\n",
    "    avg_sim = total_sim / len(embs1)\n",
    "    return avg_sim * 100  # convert to %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a71556-5ab8-4c83-92b6-c507d4f7b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x544 1 person, 1 tie, 292.2ms\n",
      "Speed: 12.5ms preprocess, 292.2ms inference, 14.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x416 1 person, 1 tie, 218.4ms\n",
      "Speed: 5.2ms preprocess, 218.4ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Object-Level Similarity: 86.46%\n"
     ]
    }
   ],
   "source": [
    "# Get crop-wise embeddings from both images\n",
    "crop_embs1 = get_clip_crops_embeddings( \"image_pairs/boy1.jpg\")\n",
    "crop_embs2 = get_clip_crops_embeddings( \"image_pairs/boy2.jpg\")\n",
    "\n",
    "obj_level_sim = compute_object_level_similarity(crop_embs1, crop_embs2)\n",
    "print(f\"Object-Level Similarity: {obj_level_sim:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ce4d25-3bcb-43d5-84de-f01414651974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Similarity Score: 87.11%\n"
     ]
    }
   ],
   "source": [
    "final_similarity = 0.5 * clip_sim * 100 + 0.5 * obj_level_sim\n",
    "print(f\"Final Similarity Score: {final_similarity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecbb11-1b20-4952-b370-5ac66dfaf491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f104aa2-eae1-4885-bfc9-60127e5cf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese MLP head to learn similarity from CLIP embeddings\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SiameseHead(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output: similarity score between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, emb1, emb2):\n",
    "        x = torch.cat([emb1, emb2], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab64a0-5b51-484b-97f0-b30edb52e8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60409f38-5494-4de4-b154-fcceae9bffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, processor, device):\n",
    "        self.pairs_df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs_df.iloc[idx]\n",
    "        path1 = os.path.join(self.image_dir, row['image1'])\n",
    "        path2 = os.path.join(self.image_dir, row['image2'])\n",
    "        label = float(row['similarity'])\n",
    "\n",
    "\n",
    "        # Load both images\n",
    "        img1 = Image.open(path1).convert(\"RGB\")\n",
    "        img2 = Image.open(path2).convert(\"RGB\")\n",
    "\n",
    "        # Use CLIP processor to get tensors\n",
    "        inputs1 = self.processor(images=img1, return_tensors=\"pt\")\n",
    "        inputs2 = self.processor(images=img2, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings from frozen CLIP\n",
    "        with torch.no_grad():\n",
    "            emb1 = clip_model.get_image_features(**{k: v.to(self.device) for k, v in inputs1.items()})\n",
    "            emb2 = clip_model.get_image_features(**{k: v.to(self.device) for k, v in inputs2.items()})\n",
    "\n",
    "        # Normalize\n",
    "        emb1 = emb1 / (emb1.norm(dim=1, keepdim=True) + 1e-10)\n",
    "        emb2 = emb2 / (emb2.norm(dim=1, keepdim=True) + 1e-10)\n",
    "\n",
    "        return emb1.squeeze(0), emb2.squeeze(0), torch.tensor([label], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f026b-5ad3-4893-bc64-a114b7e98942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d27c42-7b85-4bfd-b912-67bc97952f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 13/13 [00:35<00:00,  2.72s/it, loss=0.656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 13/13 [00:35<00:00,  2.77s/it, loss=0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.5935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 13/13 [00:35<00:00,  2.69s/it, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 13/13 [00:35<00:00,  2.73s/it, loss=0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.5575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 13/13 [00:35<00:00,  2.73s/it, loss=0.537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5367\n",
      "Training complete ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImagePairDataset(\"pairs.csv\", \"image_pairs\", clip_proc, device)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the Siamese head and move to device\n",
    "siamese_head = SiameseHead().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(siamese_head.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    siamese_head.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for emb1, emb2, labels in pbar:\n",
    "        emb1, emb2, labels = emb1.to(device), emb2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_head(emb1, emb2).squeeze()  # output shape: (batch_size)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876653d2-507f-42ce-ad3d-9aaa0173cc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60870ca5-ae61-46c6-a9ef-3404a325eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_similarity(img_path1, img_path2):\n",
    "    img1 = Image.open(img_path1).convert(\"RGB\")\n",
    "    img2 = Image.open(img_path2).convert(\"RGB\")\n",
    "\n",
    "    inputs1 = clip_proc(images=img1, return_tensors=\"pt\").to(device)\n",
    "    inputs2 = clip_proc(images=img2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb1 = clip_model.get_image_features(**inputs1)\n",
    "        emb2 = clip_model.get_image_features(**inputs2)\n",
    "\n",
    "    emb1 = emb1 / (emb1.norm(dim=1, keepdim=True) + 1e-10)\n",
    "    emb2 = emb2 / (emb2.norm(dim=1, keepdim=True) + 1e-10)\n",
    "\n",
    "    siamese_head.eval()\n",
    "    with torch.no_grad():\n",
    "        sim_score = siamese_head(emb1, emb2).item()\n",
    "\n",
    "    print(f\"Similarity: {sim_score * 100:.2f}%\")\n",
    "    return sim_score * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02028ac6-ad10-4836-bc95-3c03032c91b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "dataset = ImagePairDataset(\"pairs.csv\", \"image_pairs\", clip_proc, device)\n",
    "emb1, emb2, label = dataset[0]\n",
    "print(emb1.shape, emb2.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39bd9a34-88e9-4cc3-a4ed-49110607a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "dataset = ImagePairDataset(\"pairs.csv\", \"image_pairs\", clip_proc, device)\n",
    "emb1, emb2, label = dataset[0]\n",
    "print(emb1.shape, emb2.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b80406-07a0-42c7-bd0a-f5c340c89b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 35.97%\n",
      "Predicted similarity: 35.97%\n"
     ]
    }
   ],
   "source": [
    "sim_score = predict_similarity(\"image_pairs/boy1.jpg\", \"image_pairs/boy1_zoomed.jpg\")\n",
    "print(f\"Predicted similarity: {sim_score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c917d1b-aa78-404a-b881-6b5561f5c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 35.97%\n",
      "\n",
      "0: 640x544 1 person, 1 tie, 369.5ms\n",
      "Speed: 47.8ms preprocess, 369.5ms inference, 29.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x384 1 person, 236.9ms\n",
      "Speed: 5.1ms preprocess, 236.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Combined similarity score: 57.21%\n"
     ]
    }
   ],
   "source": [
    "def combined_similarity(img_path1, img_path2, weight_siamese=0.5, weight_yolo=0.5):\n",
    "    # Siamese similarity\n",
    "    sim_score_siamese = predict_similarity(img_path1, img_path2)\n",
    "    \n",
    "    # YOLO crop similarity\n",
    "    crop_embs1 = get_clip_crops_embeddings(img_path1)\n",
    "    crop_embs2 = get_clip_crops_embeddings(img_path2)\n",
    "    obj_level_sim = compute_object_level_similarity(crop_embs1, crop_embs2)\n",
    "    \n",
    "    # Weighted average\n",
    "    combined_score = weight_siamese * sim_score_siamese + weight_yolo * obj_level_sim\n",
    "    return combined_score\n",
    "\n",
    "# Example usage:\n",
    "score = combined_similarity(\"image_pairs/boy1.jpg\", \"image_pairs/boy1_zoomed.jpg\")\n",
    "print(f\"Combined similarity score: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85dcef-966d-4f68-bc90-6d9803143bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
